{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime as dt\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import gzip\n",
    "import json\n",
    "import imp\n",
    "\n",
    "import data\n",
    "import train\n",
    "import stats\n",
    "import plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_network, get_net_from_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stats import calc_datapoint_statistics, calc_global_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_dataset, compute_idx_splits, scale_to_standard, get_dir_files, load_method_dict, compute_pca_projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot import plot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Timestamp of the format: hour:minute:second \"\"\"                  \n",
    "def timestamp(dt_obj):\n",
    "    return \"%d_%d_%d_%d_%d_%d\" % (dt_obj.year, dt_obj.month, dt_obj.day, dt_obj.hour, dt_obj.minute, dt_obj.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_datasets = {'boston', 'concrete', 'energy', 'abalone', 'naval', \n",
    "                      'power', 'protein', 'wine_red', 'yacht', 'year', \n",
    "                      'california', 'diabetes', 'superconduct',\n",
    "                    'toy_modulated', 'toy_hf'}\n",
    "\n",
    "toy_datasets = {'toy_hf','toy_modulated'}\n",
    "small_datasets = {'toy_hf','yacht', 'diabetes', 'boston', 'energy', 'concrete', 'wine_red'}\n",
    "large_datasets = {'toy_modulated', 'kin8nm', 'abalone', 'naval', 'power', 'superconduct', 'protein', 'california'}\n",
    "very_large_datasets = {'year'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "available_splits = {'random_folds', 'single_random_split', 'single_label_split', 'label_folds', 'single_pca_split', 'pca_folds'}\n",
    "available_methods = {'vanilla', 'de', 'pu', 'mc_wd=0.000001', 'pu_de', 'mc_pu', 'swag', 'evidential', 'concrete_dropout', 'new_wdrop_exact_l=5'}\n",
    "\n",
    "dt_now = dt.datetime.now()\n",
    "exp_ident = 'TEST' # TODO: SPECIFY IDENTIFIER HERE\n",
    "exp_dir = './experiment_results/%s_%s' % (exp_ident, timestamp(dt_now))\n",
    "reuse_exp_dir = False\n",
    "\n",
    "# Base parameters\n",
    "n_output = 1\n",
    "\n",
    "net_params = {'n_output': n_output,\n",
    "            'layer_width': 100,\n",
    "            'num_layers': 2,\n",
    "            'nonlinearity': torch.nn.ReLU(), #tanh,sigmoid\n",
    "            'init_corrcoef':0.0,\n",
    "            'de_components': 5} \n",
    "\n",
    "train_params = {'device': 'cpu', #torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "              'drop_bool':True,\n",
    "              'drop_bool_ll':True,\n",
    "              'drop_p':0.1,\n",
    "              'num_epochs': 45,#00,\n",
    "              'batch_size': 100,\n",
    "              'learning_rate': 0.001,\n",
    "              'loss_func':torch.nn.MSELoss(reduction='mean'),\n",
    "              'weight_decay': 0,\n",
    "              'loss_params':[5,1,False]}\n",
    "\n",
    "use_splits = available_splits\n",
    "datasets = ['yacht', 'diabetes', 'boston', 'energy', 'concrete', 'wine_red', 'abalone', 'kin8nm', 'power', 'naval', 'california', 'superconduct', 'protein', 'year']\n",
    "methods = ['swag', 'de', 'pu', 'evidential', 'pu_de', 'mc_pu', 'mc_wd=0.000001', 'concrete_dropout', 'new_wdrop_exact_l=5']\n",
    "\n",
    "with_valset = False\n",
    "folds_with_val = False\n",
    "val_perc = 0.\n",
    "train_perc = 0.8 # 0.8\n",
    "if with_valset:\n",
    "    folds_with_val = True # False\n",
    "    val_perc = 0.2 # 0\n",
    "    train_perc = 0.7 # 0.8\n",
    "\n",
    "eval_epochs = False\n",
    "\n",
    "\n",
    "def _local_stats(net, method, X_train, y_train, X_test, y_test, method_dict, \n",
    "                 X_val=None, y_val=None, train_projections=None, test_projections=None, val_projections=None, epoch=None):\n",
    "    \n",
    "    if epoch is not None:\n",
    "        if epoch not in method_dict:\n",
    "            method_dict[epoch] = dict()\n",
    "            \n",
    "        _local_stats(net, method, X_train, y_train, X_test, y_test, method_dict[epoch],\n",
    "                    X_val=X_val, y_val=y_val, train_projections=train_projections, test_projections=test_projections, val_projections=val_projections,\n",
    "                    epoch=None)\n",
    "        return\n",
    "    \n",
    "    iso_reg_train = []\n",
    "    df_train = calc_datapoint_statistics(net=net,data=[X_train, y_train], method=method, iso_reg=iso_reg_train, train_N=len(X_train))\n",
    "    iso_reg = iso_reg_train\n",
    "\n",
    "    if with_valset:\n",
    "        iso_reg_val = []\n",
    "        df_val = calc_datapoint_statistics(net=net,data=[X_val, y_val],  method=method, iso_reg=iso_reg_val, train_N=len(X_train))\n",
    "        iso_reg = iso_reg_val\n",
    "\n",
    "    df_test  = calc_datapoint_statistics(net=net,data=[X_test, y_test],  method=method, iso_reg=iso_reg, train_N=len(X_train))\n",
    "\n",
    "    if train_projections is not None:\n",
    "        df_train['pca0_projection'] = train_projections #tmp[train_idxs].values\n",
    "    if test_projections is not None:\n",
    "        df_test['pca0_projection']  = test_projections #tmp[test_idxs].values\n",
    "\n",
    "    if with_valset:\n",
    "        if val_projections is not None:\n",
    "            df_val['pca0_projection'] = val_projections\n",
    "        method_dict[method] = [df_train, df_val, df_test]\n",
    "    else:\n",
    "        method_dict[method] = [df_train, df_test]\n",
    "\n",
    "def _global_stats(method_dict):\n",
    "    global_stats = {}\n",
    "    for method in method_dict:\n",
    "        split_idx_list = [0, 1, 2] if with_valset else [0, 1]\n",
    "        global_stats[method] = [calc_global_statistics(method_dict[method][i], n_output=n_output) for i in split_idx_list]\n",
    "    return global_stats\n",
    "            \n",
    "\n",
    "def _method_dict_to_json(method_dict):\n",
    "    \n",
    "    method_dict_json = dict()\n",
    "    for method in method_dict:\n",
    "        method_item = method_dict[method]\n",
    "        \n",
    "        method_dict_json[method] = []\n",
    "        for item in method_item:\n",
    "            if isinstance(item, list): # item is list of Dataframes\n",
    "                method_dict_json[method].append([df.to_json() for df in item])\n",
    "            else: # item is Dataframe\n",
    "                method_dict_json[method].append(item.to_json())\n",
    "    \n",
    "    return method_dict_json\n",
    "\n",
    "def _read_splits_from_exp_dir(exp_dir, dataset_id):\n",
    "    \n",
    "    dir_files = get_dir_files(exp_dir, dataset_id)\n",
    "    split_modes = dir_files['data_dict'].keys()\n",
    "    \n",
    "    splits = {}\n",
    "    for split_mode in split_modes:\n",
    "        \n",
    "        splits[split_mode] = []\n",
    "        for fold_idx in sorted(dir_files['data_dict'][split_mode]):\n",
    "            with gzip.open(dir_files['data_dict'][split_mode][fold_idx]) as f:\n",
    "                data_dict = json.load(f)\n",
    "            \n",
    "            if with_valset:\n",
    "                splits[split_mode].append((data_dict['train_idxs'], data_dict['val_idxs'], data_dict['test_idxs']))\n",
    "            else:\n",
    "                splits[split_mode].append((data_dict['train_idxs'], data_dict['test_idxs']))\n",
    "            \n",
    "    return splits\n",
    "\n",
    "def _store_model(net_dict, net, method_identifier):\n",
    "    \n",
    "    if isinstance(net, list):\n",
    "        for i, subnet in enumerate(net):\n",
    "            net_dict['%s_sub=%d' % (method_identifier, i)] = copy.deepcopy(subnet.state_dict())\n",
    "    else:\n",
    "        net_dict[method_identifier] = copy.deepcopy(net.state_dict())\n",
    "            \n",
    "if reuse_exp_dir and datasets is None:\n",
    "    datasets = [ds for ds in os.listdir(exp_dir) if ds in available_datasets]\n",
    "    print(\"Reusing datasets: \", datasets)\n",
    "    \n",
    "start_ = time.time()\n",
    "for dataset_id in datasets: \n",
    "    \n",
    "    X, y = load_dataset(dataset_id)\n",
    "    n_feat = X.shape[1]\n",
    "    \n",
    "    net_params_ = dict(net_params)\n",
    "    train_params_ = dict(train_params)\n",
    "    \n",
    "    if dataset_id in very_large_datasets:\n",
    "        fold_idxs = [0, 3, 5, 7, 9]\n",
    "        split_idxs = [spl for spl in use_splits if spl in ['random_folds', 'label_folds', 'pca_folds']]\n",
    "        train_params_['num_epochs'] = 150\n",
    "        train_params_['batch_size'] = 500\n",
    "    \n",
    "    elif dataset_id in large_datasets:\n",
    "        fold_idxs = [0, 3, 5, 7, 9]\n",
    "        split_idxs = [spl for spl in available_splits if spl in use_splits]\n",
    "        train_params_['num_epochs'] = 150 \n",
    "    else:\n",
    "        fold_idxs = list(range(10))\n",
    "        split_idxs = [spl for spl in available_splits if spl in use_splits]\n",
    "    \n",
    "    reuse_exp_dir_dataset = reuse_exp_dir\n",
    "    if reuse_exp_dir_dataset:\n",
    "        try:\n",
    "            splits = _read_splits_from_exp_dir(exp_dir, dataset_id)\n",
    "            projections = compute_pca_projections(X)\n",
    "        except FileNotFoundError:\n",
    "            reuse_exp_dir_dataset = False\n",
    "    \n",
    "    if not reuse_exp_dir_dataset:\n",
    "        splits = compute_idx_splits(X, y, fold_idxs=fold_idxs, splits=split_idxs, train_perc=train_perc, val_perc=val_perc, folds_with_val=folds_with_val) # use 10-folds\n",
    "        projections = splits['projections']\n",
    "    \n",
    "    for split_mode in splits:\n",
    "        \n",
    "        if split_mode == 'projections':\n",
    "            continue\n",
    "        \n",
    "        folds = splits[split_mode]        \n",
    "        if (type(folds) == tuple) and (len(folds) in [2, 3]):\n",
    "            folds = [folds]\n",
    "        \n",
    "        for fold_idx, split_idxs in enumerate(folds):\n",
    "            \n",
    "            if with_valset:\n",
    "                train_idxs, val_idxs, test_idxs = split_idxs\n",
    "            else:\n",
    "                train_idxs, test_idxs = split_idxs\n",
    "            \n",
    "            identifier = 'dataset=%s_splitmode=%s_foldidx=%d' % (dataset_id, split_mode, fold_idx)\n",
    "            \n",
    "            X_train = X[train_idxs]\n",
    "            X_test = X[test_idxs]\n",
    "            y_train = y[train_idxs]\n",
    "            y_test = y[test_idxs]\n",
    "            train_projections, test_projections = projections[train_idxs], projections[test_idxs]\n",
    "            \n",
    "            if with_valset:\n",
    "                X_val = X[val_idxs]\n",
    "                y_val = y[val_idxs]\n",
    "                val_projections = projections[val_idxs]\n",
    "                X_train, y_train, X_test, y_test, X_val, y_val, X_scaler, y_scaler = scale_to_standard(X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "            else:\n",
    "                X_train, y_train, X_test, y_test, _, _, X_scaler, y_scaler = scale_to_standard(X_train, y_train, X_test, y_test)\n",
    "            \n",
    "            # choose a bunch of uncertainty methods and train the respective models \n",
    "            method_dict = {}\n",
    "            method_dict_json = {}\n",
    "            method_dict_epochs = {}\n",
    "            net_dict = {}\n",
    "            for method in methods:\n",
    "                \n",
    "                method_identifier = '%s_method=%s' % (identifier, method)\n",
    "                print(method_identifier)\n",
    "                \n",
    "                net = get_net_from_method(method, n_feat, len(X_train), net_params_, train_params_, n_output=n_output) \n",
    "                print(net_params_, train_params_)\n",
    "                \n",
    "\n",
    "                if eval_epochs:\n",
    "                    train_network(net=net, data=[X_train, y_train], train_params=train_params_, method=method, \n",
    "                                  epoch_callback=lambda net_, epoch: _local_stats(net_, method, X_train, y_train, X_test, y_test, method_dict_epochs,\n",
    "                                                                     train_projections=train_projections, test_projections=test_projections, epoch=epoch))\n",
    "                else:\n",
    "                    train_network(net=net, data=[X_train, y_train], train_params=train_params_, method=method)\n",
    "\n",
    "\n",
    "                if with_valset:\n",
    "                    _local_stats(net, method, X_train, y_train,  X_test, y_test, method_dict, \n",
    "                                 X_val, y_val, train_projections, test_projections, val_projections)\n",
    "                else:\n",
    "                    _local_stats(net, method, X_train, y_train, X_test, y_test, method_dict,\n",
    "                                 train_projections=train_projections, test_projections=test_projections)\n",
    "\n",
    "                _store_model(net_dict, net, method_identifier)\n",
    "            \n",
    "            exp_dataset_dir = '%s/%s' % (exp_dir, dataset_id)\n",
    "            os.makedirs(exp_dataset_dir, exist_ok=True)\n",
    "            \n",
    "            if not reuse_exp_dir_dataset:\n",
    "                data_dict = {'X_mean': X_scaler.mean_.tolist(), 'X_scale': X_scaler.scale_.tolist(), 'y_mean': y_scaler.scale_.tolist(), 'y_scale': y_scaler.scale_.tolist(), \n",
    "                             'train_idxs': train_idxs.tolist(), 'test_idxs': test_idxs.tolist()}\n",
    "                if with_valset:\n",
    "                    data_dict['val_idxs'] = val_idxs.tolist()\n",
    "    \n",
    "                with gzip.open('%s/data_dict_%s.json.zip' % (exp_dataset_dir, identifier), 'wt', encoding='ascii') as fp:\n",
    "                    json.dump(data_dict, fp)\n",
    "            \n",
    "            if reuse_exp_dir_dataset:\n",
    "                dir_files = get_dir_files(exp_dir, dataset_id)\n",
    "                prev_method_dict = load_method_dict(dir_files, split_mode, folds=[fold_idx])[0]\n",
    "                prev_method_dict.update(method_dict)\n",
    "                method_dict = prev_method_dict\n",
    "            \n",
    "            method_dict_json = _method_dict_to_json(method_dict)\n",
    "            with gzip.open('%s/method_dict_%s.json.zip' % (exp_dataset_dir, identifier), 'wt', encoding='ascii') as fp:\n",
    "                json.dump(method_dict_json, fp)\n",
    "            \n",
    "            if n_output == 1:\n",
    "                if not reuse_exp_dir_dataset:\n",
    "                    plot_results(method_dict, '%s/%s.png' % (exp_dataset_dir, identifier), with_valset=with_valset)\n",
    "                          \n",
    "            # print global statistics for the different methods (for both train and test)\n",
    "            global_stats = _global_stats(method_dict)\n",
    "            with gzip.open('%s/global_stats_%s.json.zip' % (exp_dataset_dir, identifier), 'wt', encoding='ascii') as fp:\n",
    "                json.dump(global_stats, fp)\n",
    "            \n",
    "            if eval_epochs:\n",
    "                if reuse_exp_dir_dataset:\n",
    "                    dir_files = get_dir_files(exp_dir, dataset_id)\n",
    "                    prev_method_dict_epochs = load_method_dict(dir_files, dataset_id, folds=[fold_idx], epochs=True)\n",
    "                    for epoch in method_dict_epochs:\n",
    "                        if epoch in prev_method_dict_epochs:\n",
    "                            prev_method_dict_epochs[epoch].update(method_dict_epochs[epoch])\n",
    "                        else:\n",
    "                            prev_method_dict_epochs[epoch] = method_dict_epochs[epoch]\n",
    "                    method_dict_epochs = prev_method_dict_epochs\n",
    "                \n",
    "                global_stats_epochs = {epoch: _global_stats(method_dict_epochs[epoch]) for epoch in method_dict_epochs}\n",
    "                with gzip.open('%s/global_stats_epochs_%s.json.zip' % (exp_dataset_dir, identifier), 'wt', encoding='ascii') as fp:\n",
    "                    json.dump(global_stats_epochs, fp)\n",
    "            \n",
    "            if reuse_exp_dir_dataset:\n",
    "                dir_files = get_dir_files(exp_dir, dataset_id)\n",
    "                prev_net_dict = torch.load(dir_files['model'][split_mode][fold_idx])\n",
    "                prev_net_dict.update(net_dict)\n",
    "                net_dict = prev_net_dict\n",
    "            \n",
    "            torch.save(net_dict, '%s/model_%s.pt' % (exp_dataset_dir, identifier))\n",
    "\n",
    "print(time.time() - start_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torch-env] *",
   "language": "python",
   "name": "conda-env-.conda-torch-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}